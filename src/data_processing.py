import pandas as pd
import cfgrib
import sqlite3
import os
import numpy as np
import calendar
from datetime import datetime
import joblib

from src.data_retrieve import add_cfs_to_db
from src.calculations import calculate_evaporation


def shift_variables(df, lag=0, lead=0):
    """
    Create the variables columns to include lags (last month values) and lead variables
    
    Parameters:
    - df (pd.DataFrame): The DataFrame containing the time series data.
    - lag (int): The number of months you want to include lagged variables. Default = 0
    - lead (int): The number of months for the advance variables. Default = 0
    
    Returns:
    - pd.DataFrame: The DataFrame with added variable columns for lags and leading.
    """
    df = df.copy()  # To avoid modifying the original DataFrame

    new_columns = []  # List to store the new lag and lead columns

    # Generate target columns for the lag and lead months
    for column in df.columns:
        for lag_month in range(1, lag + 1):
            new_columns.append(df[column].shift(lag_month).rename(f'{column}_mo-{lag_month}'))
        for lead_month in range(1, lead + 1):
            new_columns.append(df[column].shift(-lead_month).rename(f'{column}_mo{lead_month}'))

    # Concatenate the new columns with the original DataFrame
    df = pd.concat([df] + new_columns, axis=1)

    # Drop rows with any NaN values generated by shifting for the target
    df = df.dropna()

    return df

def add_cfs_to_db(database, table, cfs_run, year, month, lake, surface_type, cnbs, value):
    conn = sqlite3.connect(database)
    cursor = conn.cursor()

    # Properly insert the table name using f-string or str.format() outside of the SQL statement
    query = f'''
    INSERT OR REPLACE INTO {table} (
        cfs_run, year, month, lake, surface_type, cnbs, value
    ) VALUES (?, ?, ?, ?, ?, ?, ?)
    '''

    # Insert the data into the table
    cursor.execute(query, (cfs_run, year, month, lake, surface_type, cnbs, value))

    conn.commit()
    conn.close()

def process_grib_files(download_dir, database, table, cfs_run, mask_lat, mask_lon, mask_ds, mask_variables, area):

    # Check to see if there are outdated index files and remove them before reopening the grib2 file
    # This is not required but if you don't delete them first, you will get a warning as it opens each file
    for idx_file in [f for f in os.listdir(download_dir) if f.endswith('.idx')]:
        #print(f'Removing outdated index file: {idx_file}')
        os.remove(os.path.join(download_dir, idx_file))

    # Find all the .grb2 files in the directory
    pgb_list = sorted(file for file in os.listdir(download_dir) if file.startswith(f'pgbf.01.{cfs_run}') and file.endswith('grb2'))
    for filename in pgb_list:
    
        pgb_file = f'{download_dir}{filename}'

        parts = filename.split('.')
        cfs_run = int(parts[2])
        forecast = parts[3]  # Assuming parts[2] is in the format YYYYMM
        forecast_year = int(forecast[:4])
        forecast_month = int(forecast[4:6])

        _, num_days = calendar.monthrange(forecast_year, forecast_month)

        ## Precipitation ##

        pgb_surface = cfgrib.open_dataset(pgb_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'}, decode_timedelta=False)
        pcp = pgb_surface['tp']  # Total precipitation
        
        # Cut the variable to the mask domain
        pcp_cut = pcp.sel(
            latitude=slice(mask_lat.max(), mask_lat.min()),
            longitude=slice(mask_lon.min(), mask_lon.max())
        )

        # Remap and upscale the variable to match the mask domain
        pcp_remap = pcp_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')

        for mask_var in mask_variables:
            mask = mask_ds.variables[mask_var][:]
            
            total_pcp = (np.sum(pcp_remap * mask * area)) * 4 * num_days # pcp is for 6 hours - convert to monthly
            pcp_mm = total_pcp / np.sum(mask * area) # kg/m2 or mm

            lake_abv, surface_type = mask_var.split('_')
            if lake_abv == 'eri': lake = 'erie'
            elif lake_abv == 'ont': lake = 'ontario'
            elif lake_abv == 'sup': lake = 'superior'
            elif lake_abv == 'mih': lake = 'michigan-huron'
            else: print("Error: The mask variables need to begin with 'eri', 'ont', 'sup', or 'mih'. Check the mask file and try again.")

            # Insert data to dataframe
            add_cfs_to_db(database, table, cfs_run, forecast_year, forecast_month, lake, surface_type, 'precipitation', pcp_mm.item())

        ## 2 m Temperature ##

        # Open the corresponding flx file
        flx_file = f"{download_dir}flxf.01.{cfs_run}.{forecast}.avrg.grib.grb2"

        try:
            # Open the flx file at the 2m level to pull the 2m air temperature
            flx_2mabove = cfgrib.open_dataset(flx_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'heightAboveGround', 'level': 2}, decode_timedelta=False)
            try:
                mean2t = flx_2mabove['mean2t']
            except KeyError:
                print("mean2t not found in flux file, trying avg_2t.")
                mean2t = flx_2mabove['avg_2t']

            # Cut the variable to the mask domain
            mean2t_cut = mean2t.sel(
                latitude=slice(mask_lat.max(), mask_lat.min()),
                longitude=slice(mask_lon.min(), mask_lon.max())
            )
            # Remap and upscale the variable to match the mask domain
            mean2t_remap = mean2t_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')
            
            # Calculate mean2t for each of the mask variables (i.e., eri_lake, eri_basin, etc.)
            for mask_var in mask_variables:

                # Have to change the mask from fractional to all 1s and 0s
                mask_tmp = mask_ds.variables[mask_var][:]
                mask = np.ma.masked_where(np.isnan(mask_tmp), np.ones_like(mask_tmp))

                # Take the mean over the mask area
                tmp_avg = np.mean(mean2t_remap * mask)

                lake_abv, surface_type = mask_var.split('_')
                if lake_abv == 'eri': lake = 'erie'
                elif lake_abv == 'ont': lake = 'ontario'
                elif lake_abv == 'sup': lake = 'superior'
                elif lake_abv == 'mih': lake = 'michigan-huron'
                else: print("Error: The mask variables need to begin with 'eri', 'ont', 'sup', or 'mih'. Check the mask file and try again.")

                # Insert data to dataframe
                add_cfs_to_db(database, table, cfs_run, forecast_year, forecast_month, lake, surface_type, 'air_temperature', tmp_avg.item())

            ## Evaporation ##

            # Open the flx file again but at the surface level to pull the latent heat flux
            flx_surface = cfgrib.open_dataset(flx_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'}, decode_timedelta=False)
            try:
                mslhf = flx_surface['mslhf']
            except KeyError:
                print("mslhf not found in flux file, trying avg_slhtf.")
                mslhf = flx_surface['avg_slhtf']
            
            # Cut the variable to the mask domain
            mslhf_cut = mslhf.sel(
                latitude=slice(mask_lat.max(), mask_lat.min()),
                longitude=slice(mask_lon.min(), mask_lon.max())
            )
            # Remap and upscale the variable to match the mask domain
            mslhf_remap = mslhf_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')

            # Calculate evaporation across the entire domain using air temp and latent heat flux
            evap = calculate_evaporation(mean2t_remap, mslhf_remap)

            # Calculate evaporation for each of the mask variables (i.e., eri_lake, eri_land, etc.)
            for mask_var in mask_variables:
                
                mask = mask_ds.variables[mask_var][:]
                total_evap = (np.sum(evap * area * mask)) * num_days * 86400 # total in kg/sm2 * num of seconds in a month = kg/m2 or mm
                evap_mm = total_evap / np.sum(mask * area)

                lake_abv, surface_type = mask_var.split('_')
                if lake_abv == 'eri': lake = 'erie'
                elif lake_abv == 'ont': lake = 'ontario'
                elif lake_abv == 'sup': lake = 'superior'
                elif lake_abv == 'mih': lake = 'michigan-huron'
                else: print("Error: The mask variables need to begin with 'eri', 'ont', 'sup', or 'mih'. Check the mask file and try again.")

                # Insert data to dataframe
                add_cfs_to_db(database, table, cfs_run, forecast_year, forecast_month, lake, surface_type, 'evaporation', evap_mm.item())

        except FileNotFoundError: print(f"Error: The flx file that cooresponds to {pgb_file} does not exist. Please check the dataset is complete and try again.")

def predict_cnbs(X, x_scaler, y_scaler, models_info, model_name):
    # Standardize the data
    x_scaler, y_scaler = joblib.load(x_scaler), joblib.load(y_scaler)
    X_scaled = x_scaler.transform(X)

    # Find the model path by filtering the list of models
    model_info = next((model for model in models_info if model['model'] == model_name), None)
    if not model_info:
        print('Model name is not recognized.')
        return None

    model_loaded = joblib.load(model_info['path'])
    y_pred_scaled = model_loaded.predict(X_scaled)
    y_pred = y_scaler.inverse_transform(y_pred_scaled)

    column_names = ['superior_evaporation', 'superior_precipitation', 'superior_runoff',
                    'erie_evaporation', 'erie_precipitation', 'erie_runoff',
                    'ontario_evaporation', 'ontario_precipitation', 'ontario_runoff',
                    'michigan-huron_evaporation', 'michigan-huron_precipitation', 'michigan-huron_runoff']
    
    # Create DataFrame from predictions and reset index
    df = pd.DataFrame(y_pred, columns=column_names, index=X.index)

    # Calculate CNBS for all the lakes
    for lake in ['superior', 'erie', 'ontario', 'michigan-huron']:
        df[f'{lake}_cnbs'] = df[f'{lake}_precipitation'] + df[f'{lake}_runoff'] - df[f'{lake}_evaporation']

    return df

def format_predictions_and_add_to_db(database, table, df, model_name):
    # Melt the DataFrame and split 'lake_cnbs' into 'lake' and 'cnbs'
    df_reset = df.reset_index()

    # Melt the DataFrame with the columns as id_vars
    df_melted = df_reset.melt(id_vars=['cfs_run', 'month', 'year'], var_name='lake_cnbs', value_name='value')
    df_melted[['lake', 'cnbs']] = df_melted['lake_cnbs'].str.split('_', expand=True)

    # Add model column and rearrange
    df_melted['model'] = model_name
    df_melted = df_melted.drop(columns=['lake_cnbs'])[['cfs_run', 'month', 'year', 'model', 'lake', 'cnbs', 'value']]

    # Sort and set index
    df_melted = df_melted.sort_values(by=['cfs_run', 'month', 'year', 'lake']).set_index(['cfs_run', 'month', 'year'])

    # Create a connection to the SQLite database
    conn = sqlite3.connect(database)

    # Send the DataFrame to the database
    df_melted.to_sql(table, conn, if_exists='append', index=True)

    # Close the connection
    conn.commit()
    conn.close()

def filter_predictions(df):
    current_day = datetime.now().day
    current_month = datetime.now().month
    current_year = datetime.now().year

    pred_year = df.index.get_level_values('year')
    pred_month = df.index.get_level_values('month')

    # Determine filtering condition based on the day of the month
    if current_day >= 26:
        # Remove this month and all previous months
        filtered_df = df[
            (pred_year > current_year) |
            (pred_year == current_year) & (pred_month > current_month)
        ]
        print(f"First month forecast: {current_month+1}")
    else:
        # Keep this month
        filtered_df = df[
            (pred_year > current_year) |
            ((pred_year == current_year) & (pred_month >= current_month))
        ]
        print(f"First month forecast: {current_month}")
    return filtered_df

def add_df_to_db(database, table, df):    
    # Create a connection to the SQLite database
    conn = sqlite3.connect(database)

    # Send the DataFrame to the database
    df.to_sql(table, conn, if_exists='append', index=True)

    # Close the connection
    conn.commit()
    conn.close()