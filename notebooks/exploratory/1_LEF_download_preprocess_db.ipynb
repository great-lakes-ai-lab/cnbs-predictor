{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/Pre-process CFS forecast data\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/28/2024\n",
    "Updated: 03/14/2024\n",
    "\n",
    "This script reads downloads CFS forecast data from the AWS as grib2 files. It then opens the grib2 files, calculates total basin, lake, and land, precipitation, evaporation, and average 2m air temperature. These calculations are then added to the running CSV files. This script needs the following files:\n",
    "\n",
    "- GL_mask.nc\n",
    "- cfs_forecast_data.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the src directory (two levels up)\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from importlib import reload\n",
    "import src.misc\n",
    "reload(src.misc)\n",
    "\n",
    "from src.data_retrieve import *\n",
    "from src.data_processing import process_grib_files\n",
    "from src.calculations import calculate_grid_cell_areas\n",
    "from src.misc import create_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where you cloned the repo\n",
    "path_to_repo = '/Users/ljob/Desktop/'\n",
    "\n",
    "# Path to download data to\n",
    "download_dir = path_to_repo + 'cnbs-predictor/data/CFS/'\n",
    "\n",
    "# Path to input files\n",
    "input_dir = path_to_repo + 'cnbs-predictor/data/input/'\n",
    "\n",
    "# Location of the mask file\n",
    "mask_file = input_dir + 'GL_mask.nc'\n",
    "\n",
    "# Location of database with CFS forecast data\n",
    "database = input_dir + 'cfs_forecast_data.db'\n",
    "\n",
    "# Where would you like to pull the data from?\n",
    "source = 'aws' # 'aws' or 'ncei'\n",
    "\n",
    "# Do you need to download CFS data?\n",
    "download_cfs = 'no'\n",
    "\n",
    "# Do you want to process the CFS data and \n",
    "process_cfs = 'yes'\n",
    "\n",
    "# Delete grib files after processing and saving data?\n",
    "delete_files = 'no'\n",
    "\n",
    "# Auto 'yes' will open the existing files, pull the last date to determine the start date and yesterday becomes the end date in\n",
    "# order to make the csvs up-to-date\n",
    "auto = 'no'\n",
    "\n",
    "start_date = '03-15-2025'\n",
    "end_date = '03-16-2025'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presets\n",
    "\n",
    "These shouldn't change unless the location changes for CFS data or the user wants different files (products specifies the prefix of the files. Different files contain different variables) or a specific forecast (utc specifies the forecast time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Presets ##\n",
    "products = ['pgb','flx']\n",
    "utc = ['00','06','12','18']\n",
    "\n",
    "# Define mask variables\n",
    "mask_variables = ['eri_lake','eri_land',\n",
    "                  'ont_lake','ont_land',\n",
    "                  'mih_lake','mih_land',\n",
    "                  'sup_lake','sup_land']\n",
    "\n",
    "#AWS bucket name to locate the CFS forecast\n",
    "bucket_name = 'noaa-cfs-pds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the download directory to see if it exists or create it if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/Users/ljob/Desktop/cnbs-predictor/data/CFS/' already exists.\n"
     ]
    }
   ],
   "source": [
    "create_directory(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open existing Database or create new one if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_database(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows the user to set the script to auto. When auto = yes, the script opens one of the CSVs (temperature), reads the last date that it recorded and automatically makes the start date the next day. It then will run through yesterday's date in order to be caught up. If auto = no, then the user can input a date range. This option is convienent for testing or for starting new CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: 03-15-2025 00Z and continuing through: 03-16-2025 18Z\n"
     ]
    }
   ],
   "source": [
    "if auto == 'yes':\n",
    "        # Fetch next cfs_run date and use yesterday's date for the end date\n",
    "        start_date_i = get_next_cfs_run(database, 'cfs_forecast_data')\n",
    "        end_date_i = (datetime.now() - timedelta(days=1)).strftime(\"%m-%d-%Y\") + \" 00\"\n",
    "        # Validate dates\n",
    "        if start_date_i >= end_date_i:\n",
    "            print(\"The csv files are up-to-date.\")\n",
    "        else:\n",
    "            print(f\"Starting from: {start_date_i}Z and continuing through: {end_date_i}Z\")\n",
    "\n",
    "else:\n",
    "    # Ensure both start_date and end_date have hour info\n",
    "    start_date = (start_date + \" 00\") if len(start_date) == 10 else start_date\n",
    "    end_date = (end_date + \" 18\") if len(end_date) == 10 else end_date\n",
    "\n",
    "    # Convert to datetime objects for comparison\n",
    "    start_date_i = datetime.strptime(start_date, \"%m-%d-%Y %H\")\n",
    "    end_date_i = datetime.strptime(end_date, \"%m-%d-%Y %H\")\n",
    "\n",
    "    # Validate dates\n",
    "    if start_date_i == end_date_i:\n",
    "        print(start_date_i)\n",
    "        print(\"The csv files are up-to-date.\")\n",
    "    elif start_date_i > end_date_i:\n",
    "        print(start_date_i)\n",
    "        print(\"There is an error in the input dates. Please try again.\")\n",
    "    else:\n",
    "        print(f\"Starting from: {start_date_i.strftime('%m-%d-%Y %H')}Z and continuing through: {end_date_i.strftime('%m-%d-%Y %H')}Z\")\n",
    "\n",
    "date_array = pd.date_range(start=start_date_i, end=end_date_i, freq='6h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the mask file. Pull the latitude and longitude to be used to cut the global variable down to just the Great Lakes domain and upscale. Also calculates area of each of the grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the mask file and calculate the grid cell areas\n",
    "mask_ds = nc.Dataset(mask_file)\n",
    "mask_lat = mask_ds.variables['latitude'][:]\n",
    "mask_lon = mask_ds.variables['longitude'][:]\n",
    "area = calculate_grid_cell_areas(mask_lon, mask_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin loop to go through the user input dates. Loop creates a directory to download the CFS grib files, runs through the download_grb2_aws funtion to download and then run through the process_grib_files to do the calculations. It then saves the calculations to the CSV files, deletes the grib2 files and moves on to the next date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Files for 2025-03-15 00:00:00.\n",
      "Done with 2025-03-15 00:00:00.\n",
      "Beginning Files for 2025-03-15 06:00:00.\n",
      "Done with 2025-03-15 06:00:00.\n",
      "Beginning Files for 2025-03-15 12:00:00.\n",
      "Done with 2025-03-15 12:00:00.\n",
      "Beginning Files for 2025-03-15 18:00:00.\n",
      "Done with 2025-03-15 18:00:00.\n",
      "Beginning Files for 2025-03-16 00:00:00.\n",
      "Done with 2025-03-16 00:00:00.\n",
      "Beginning Files for 2025-03-16 06:00:00.\n",
      "Done with 2025-03-16 06:00:00.\n",
      "Beginning Files for 2025-03-16 12:00:00.\n",
      "Done with 2025-03-16 12:00:00.\n",
      "Beginning Files for 2025-03-16 18:00:00.\n",
      "Done with 2025-03-16 18:00:00.\n"
     ]
    }
   ],
   "source": [
    "for date in date_array:\n",
    "    print(f\"Beginning Files for {date}.\")\n",
    "\n",
    "    YYYY = date.strftime(\"%Y\")\n",
    "    MM = date.strftime(\"%m\")\n",
    "    DD = date.strftime(\"%d\")\n",
    "    HH = date.strftime(\"%H\")\n",
    "\n",
    "    #date = date.strftime('%Y%m%d')\n",
    "    download_path = f'{download_dir}{YYYY}{MM}{DD}/CFS/'\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # Download the grib2 files using AWS or NCEI\n",
    "    if download_cfs == 'yes':\n",
    "        for product in products:\n",
    "            if source == 'aws':\n",
    "                url_path = f'cfs.{YYYY}{MM}{DD}/{HH}/monthly_grib_01/'\n",
    "                download_grb2_aws(product, bucket_name, url_path, download_path)\n",
    "            elif source == 'ncei':\n",
    "                base_url = 'https://www.ncei.noaa.gov/data/climate-forecast-system/access/operational-9-month-forecast/monthly-means/'\n",
    "                url_path = f'{base_url}/{YYYY}/{YYYY}{MM}/{YYYY}{MM}{DD}/{YYYY}{MM}{DD}{HH}/'\n",
    "                if not url_path or not check_url_exists(url_path):\n",
    "                    print(f\"No files available for {date}.\")\n",
    "                else:\n",
    "                    download_grb2_ncei(product, url_path, download_path)\n",
    "            else:\n",
    "                print('Input source does not exist. Source must be aws or ncei.')\n",
    "    \n",
    "    if process_cfs == 'yes':\n",
    "\n",
    "        process_grib_files(download_path, database, 'cfs_forecast_data', f'{YYYY}{MM}{DD}{HH}', mask_lat, mask_lon, mask_ds, mask_variables, area)\n",
    "\n",
    "        if delete_files == 'yes':\n",
    "            os.rmdir(download_path)\n",
    "    \n",
    "    print(f'Done with {date}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close any open files before finishing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
