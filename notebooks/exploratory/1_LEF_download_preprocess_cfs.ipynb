{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/Pre-process CFS forecast data\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/28/2024\n",
    "Updated: 02/17/2024\n",
    "\n",
    "This script reads downloads CFS forecast data from the AWS as grib2 files. It then opens the grib2 files, calculates total basin, lake, and land, precipitation, evaporation, and average 2m air temperature. These calculations are then added to the running CSV files. This script needs the following files:\n",
    "\n",
    "- GL_mask.nc\n",
    "- CFS_EVAP_forecasts_Avgs_MM.csv\n",
    "- CFS_PCP_forecasts_Avgs_MM.csv\n",
    "- CFS_TMP_forecasts_Avgs_K.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import cfgrib\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import shutil\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import calendar\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where you cloned the repo\n",
    "path_to_repo = '/Users/fitzpatrick/Desktop/'\n",
    "\n",
    "# Path to download data to\n",
    "download_dir = path_to_repo + 'cnbs-predictor-1/data/input/CFS/'\n",
    "\n",
    "# Input directory\n",
    "dir = path_to_repo + 'cnbs-predictor-1/data/input/'\n",
    "\n",
    "# Location of the mask file\n",
    "mask_file = dir + 'mask_0.31.nc'\n",
    "\n",
    "# Location of existing CSV files or path/name to new CSV files\n",
    "tmp_file = dir + 'CFS_TMP_forecasts_Avgs_K.csv'\n",
    "evap_file = dir + 'CFS_EVAP_forecasts_Avgs_MM.csv'\n",
    "pcp_file = dir + 'CFS_PCP_forecasts_Avgs_MM.csv'\n",
    "\n",
    "# Where would you like to pull the data from?\n",
    "source = 'aws' # 'aws' or 'ncei'\n",
    "\n",
    "# Do you need to download CFS data?\n",
    "download_cfs = 'no'\n",
    "\n",
    "# Do you want to process the CFS data and \n",
    "process_cfs = 'yes'\n",
    "\n",
    "# Delete grib files after processing and saving data?\n",
    "delete_files = 'no'\n",
    "\n",
    "# Auto 'yes' will open the existing files, pull the last date to determine the start date and yesterday becomes the end date in\n",
    "# order to make the csvs up-to-date\n",
    "auto = 'yes'\n",
    "\n",
    "start_date = '2025-02-20'\n",
    "end_date = '2025-02-21'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presets\n",
    "\n",
    "These shouldn't change unless the location changes for CFS data or the user wants different files (products specifies the prefix of the files. Different files contain different variables) or a specific forecast (utc specifies the forecast time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Presets ##\n",
    "products = ['pgb','flx']\n",
    "utc = ['00','06','12','18']\n",
    "\n",
    "# Define mask variables\n",
    "mask_variables = ['eri_lake','eri_land',\n",
    "                 'ont_lake','ont_land',\n",
    "                 'mih_lake','mih_land',\n",
    "                 'sup_lake','sup_land']\n",
    "\n",
    "#AWS bucket name to locate the CFS forecast\n",
    "bucket_name = 'noaa-cfs-pds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions\n",
    "\n",
    "This function goes to the AWS site and downloads the needed CFS files for a given forecast day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_grb2_aws(product, bucket_name, url_path, download_dir):\n",
    "    \"\"\"\n",
    "    Download the CFS forecast from AWS\n",
    "\n",
    "    Parameters:\n",
    "    - product: 'flx' or 'pgb'\n",
    "    - bucket_name: for CFS data it is 'noaa-cfs-pds'\n",
    "    - url_path: the url path to data\n",
    "    - download_dir: location to download data to\n",
    "    \"\"\"\n",
    "    num_files_downloaded = 0\n",
    "\n",
    "    # Create a boto3 client for S3\n",
    "    s3_config = Config(signature_version=UNSIGNED)\n",
    "    s3 = boto3.client('s3', config=s3_config)\n",
    "\n",
    "    # List all objects in the specified folder path\n",
    "    continuation_token = None\n",
    "    objects = []\n",
    "\n",
    "    # Use a loop to handle pagination\n",
    "    while True:\n",
    "        list_objects_args = {'Bucket': bucket_name, 'Prefix': url_path}\n",
    "        if continuation_token:\n",
    "            list_objects_args['ContinuationToken'] = continuation_token\n",
    "\n",
    "        list_objects_response = s3.list_objects_v2(**list_objects_args)\n",
    "\n",
    "        objects.extend(list_objects_response.get('Contents', []))\n",
    "\n",
    "        if not list_objects_response.get('IsTruncated', False):\n",
    "            break\n",
    "\n",
    "        continuation_token = list_objects_response.get('NextContinuationToken')\n",
    "\n",
    "    # Iterate over each object and download if it ends with '.grb2'\n",
    "    for obj in objects:\n",
    "        key = obj['Key']\n",
    "        if product in key and key.endswith('grib.grb2'): #if key.endswith('.grb2'):\n",
    "            local_file_path = os.path.join(download_dir, os.path.relpath(key, url_path))\n",
    "\n",
    "            # Ensure the directory structure exists\n",
    "            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "\n",
    "            # Download the file\n",
    "            s3.download_file(bucket_name, key, local_file_path)\n",
    "            num_files_downloaded += 1\n",
    "\n",
    "            print(f\"Downloaded: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_grb2_ncei(product, url_path, download_dir):\n",
    "\n",
    "    # File counter\n",
    "    num_files_downloaded = 0\n",
    "\n",
    "    try:\n",
    "        response = urllib.request.urlopen(url_path)\n",
    "        html_content = response.read().decode('utf-8')\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        links = soup.find_all('a', href=lambda href: href and href.startswith(product) and href.endswith('grib.grb2'))\n",
    "        \n",
    "        for link in links:\n",
    "            file_url = url_path + link['href']\n",
    "            filename = link['href'].split('/')[-1]\n",
    "            file_path = os.path.join(download_dir, filename)\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "            print(f\"Downloaded: {filename}\")\n",
    "\n",
    "            num_files_downloaded += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to open a csv and if it does not already exist, create it and write the header to the top row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_csv(filename, mask_variables, mode='r'):\n",
    "    # Check if the file exists, and if not, create it\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, mode='w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['cfs_run', 'forecast_year', 'forecast_month'] + mask_variables)\n",
    "        print(f\"{filename} created.\")\n",
    "        return None, None\n",
    "    else:\n",
    "        # Handle reading the file (as a DataFrame)\n",
    "        if mode == 'r':\n",
    "            # Read the CSV into a DataFrame\n",
    "            df = pd.read_csv(filename)\n",
    "            return df\n",
    "        # Handle appending to the file\n",
    "        elif mode == 'a':\n",
    "            # Open for appending with CSV writer\n",
    "            csvfile = open(filename, mode='a', newline='')\n",
    "            writer = csv.writer(csvfile)\n",
    "            print(f\"{filename} opened.\")\n",
    "            return csvfile, writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to grab a specific list of files based on the prefix or suffix of a file (ie. 'pgb', '.grb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory, affix, identifier):\n",
    "    \"\"\"\n",
    "    Get a list of all GRIB2 files in the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): Path to the directory containing files.\n",
    "    - affix (str): 'prefix' or 'suffix'\n",
    "    - identifier (str):  (ie. 'pgb', 'flx', '.grb2', or '.nc')\n",
    "    Returns:\n",
    "    - List of file paths to the GRIB2 files.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if affix == 'suffix': # ends with\n",
    "            if file_name.endswith(identifier):\n",
    "                file_path = os.path.join(directory, file_name)\n",
    "                files.append(file_path)\n",
    "        elif affix == 'prefix': # begins with\n",
    "            if file_name.startswith(identifier):\n",
    "                file_path = os.path.join(directory, file_name)\n",
    "                files.append(file_path)\n",
    "    sorted_files = sorted(files)\n",
    "    return sorted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    \"\"\"Create a directory if it doesn't already exist.\"\"\"\n",
    "    try:\n",
    "        # Check if the directory already exists\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(f\"Directory '{directory}' created.\")\n",
    "        else:\n",
    "            print(f\"Directory '{directory}' already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to delete the directory with CFS grb2 files because they are not needed after calculations are saved in the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_directory(directory_path):\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"The directory {directory_path} does not exist.\")\n",
    "        return\n",
    "    try:\n",
    "        # Remove the entire directory tree\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"Successfully deleted the directory and all its contents: {directory_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting {directory_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url_exists(url):\n",
    "    try:\n",
    "        response = requests.head(url)\n",
    "        # Check if the response is OK (status code 200)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate the grid cell areas [m2] based on the mask file. This is needed to calculate total precipitation and evaporation because the units are [kg/m2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_cell_areas(lon, lat):\n",
    "    # Calculate grid cell areas\n",
    "    # Assuming lat and lon are 1D arrays\n",
    "    # Convert latitude to radians\n",
    "\n",
    "    R = 6371000.0  # Radius of Earth in meters\n",
    "    lat_rad = np.radians(lat)\n",
    "\n",
    "    # Calculate grid cell width in radians\n",
    "    dlat = np.radians(lat[1] - lat[0])\n",
    "    dlon = np.radians(lon[1] - lon[0])\n",
    "\n",
    "    # Calculate area of each grid cell in square kilometers\n",
    "    area = np.zeros((len(lat), len(lon)))\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            area[i, j] = R**2 * dlat * dlon * np.cos(lat_rad[i])\n",
    "\n",
    "    return area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate evaporation based on the 2m air temperature and latent heat flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaporation(temperature_K, latent_heat):\n",
    "    # ET = kg/(m^2*time^1) or 1 mm\n",
    "    # LE = MJ/(M^2*time^1)\n",
    "    # λ  = MJ/kg\n",
    "\n",
    "    # Latent heat of vaporization varies slightly with temperature. Allen et al. (1998) provides an equation \n",
    "    # for calculating λ with air  temperature variation. Temperature in this case must be in degrees Celcius.\n",
    "\n",
    "    # λ=2.501−(2.361×10−3)×Temp Celcius\n",
    "\n",
    "    # so for our data with Temp in Kelvin...\n",
    "\n",
    "    # λ=2.501−((2.361×10−3)×(Temp-273.15))\n",
    "\n",
    "    # Our variable_lhf is in W/m^2 or J/(m^2*time^1). In order to convert to MJ we must multiply by 10^-6 or \n",
    "    # 0.000001. Now we have lamba and variable_lhf both in terms of MJ.\n",
    "\n",
    "    # Equation below will provide an evaporation rate in kg/m2 per s. \n",
    "\n",
    "    lamda=(2.501-(0.002361*(temperature_K-273.15)))\n",
    "    evaporation_rate=((latent_heat)*0.000001)/lamda\n",
    "\n",
    "    return evaporation_rate # kg/m2 per s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to open each of the grib2 files and calculate the total precipitation, total evaporation, and average 2m air temperature over an entire basin, land, or lake for each of the Great Lakes. This uses the mask file to calculate each of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grib_files(download_dir, tmp_writer, evap_writer, pcp_writer, mask_lat, mask_lon, mask_ds, mask_variables, area):\n",
    "    # Find all the .grb2 files in the directory\n",
    "    file_list = get_files(download_dir, 'suffix', '.grb2')\n",
    "\n",
    "    for grib2_file in file_list:\n",
    "\n",
    "        filename = os.path.basename(grib2_file)\n",
    "        parts = filename.split('.')\n",
    "        cfs_run = parts[2]\n",
    "        date_part = parts[3]  # Assuming parts[2] is in the format YYYYMM\n",
    "        forecast_year = date_part[:4]\n",
    "        forecast_month = date_part[4:6]\n",
    "\n",
    "        # Check to see if the index file already exists and if it does, delete it\n",
    "        index_file = grib2_file + '*.idx'\n",
    "        if os.path.exists(index_file):\n",
    "            print(f\"Removing old index files.\")\n",
    "            os.remove(index_file)\n",
    "\n",
    "        if filename.startswith('flxf'):\n",
    "\n",
    "            ## 2 m Temperature ##\n",
    "            # Create a temporary row to store data\n",
    "            tmp_row = [cfs_run, forecast_year, forecast_month]\n",
    "\n",
    "            # Open the flx file at the 2m level to pull the 2m air temperature\n",
    "            flx_2mabove = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'heightAboveGround', 'level': 2}, decode_timedelta=False)\n",
    "            #mean2t = flx_2mabove['mean2t']\n",
    "            mean2t = flx_2mabove['avg_2t']\n",
    "\n",
    "            # Cut the variable to the mask domain\n",
    "            mean2t_cut = mean2t.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            mean2t_remap = mean2t_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "            \n",
    "            # Calculate mean2t for each of the mask variables (i.e., eri_lake, eri_basin, etc.)\n",
    "            for mask_var in mask_variables:\n",
    "\n",
    "                # Have to change the mask from fractional to all 1s and 0s\n",
    "                mask_tmp = mask_ds.variables[mask_var][:]\n",
    "                mask = np.ma.masked_where(np.isnan(mask_tmp), np.ones_like(mask_tmp))\n",
    "\n",
    "                # Take the mean over the mask area\n",
    "                tmp_avg = np.mean(mean2t_remap * mask)\n",
    "\n",
    "                # Add data to the row\n",
    "                tmp_row.append(tmp_avg.data)\n",
    "            \n",
    "            # Save the entire row to the CSV file\n",
    "            tmp_writer.writerow(tmp_row) \n",
    "\n",
    "            ## Evaporation ##\n",
    "            # Create a temporary row to store data\n",
    "            evap_row = [cfs_run, forecast_year, forecast_month]\n",
    "\n",
    "            # Open the flx file again but at the surface level to pull the latent heat flux\n",
    "            flx_surface = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'}, decode_timedelta=False)\n",
    "            #mslhf = flx_surface['mslhf']\n",
    "            mslhf = flx_surface['avg_slhtf']\n",
    "            \n",
    "            # Cut the variable to the mask domain\n",
    "            mslhf_cut = mslhf.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            mslhf_remap = mslhf_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "\n",
    "            # Calculate evaporation across the entire domain using air temp and latent heat flux\n",
    "            evap = calculate_evaporation(mean2t_remap, mslhf_remap)\n",
    "            \n",
    "            year = int(forecast_year)\n",
    "            month = int(forecast_month)\n",
    "\n",
    "            _, num_days = calendar.monthrange(year, month)\n",
    "\n",
    "            # Calculate evaporation for each of the mask variables (i.e., eri_lake, eri_land, etc.)\n",
    "            for mask_var in mask_variables:\n",
    "                \n",
    "                mask = mask_ds.variables[mask_var][:]\n",
    "                total_evap = (np.sum(evap * area * mask)) * num_days * 86400 # total in kg/sm2 * num of seconds in a month = kg/m2 or mm\n",
    "                evap_mm = total_evap / np.sum(mask * area)\n",
    "\n",
    "                # Add data to the row\n",
    "                evap_row.append(evap_mm.data)\n",
    "\n",
    "            # Save the entire row to the CSV file\n",
    "            evap_writer.writerow(evap_row)\n",
    "\n",
    "        ## Precipitation ##\n",
    "\n",
    "        elif filename.startswith('pgbf'):\n",
    "            # Create a temporary row to store data\n",
    "            pcp_row = [cfs_run, forecast_year, forecast_month]\n",
    "\n",
    "            # Open the pgb file at the surface level to pull the precipitation\n",
    "            pgb_surface = cfgrib.open_dataset(grib2_file, engine='cfgrib', filter_by_keys={'typeOfLevel': 'surface'}, decode_timedelta=False)\n",
    "  \n",
    "            pcp = pgb_surface['tp']  # Total precipitation\n",
    "            \n",
    "            # Cut the variable to the mask domain\n",
    "            pcp_cut = pcp.sel(\n",
    "                latitude=slice(mask_lat.max(), mask_lat.min()),\n",
    "                longitude=slice(mask_lon.min(), mask_lon.max())\n",
    "            )\n",
    "            # Remap and upscale the variable to match the mask domain\n",
    "            pcp_remap = pcp_cut.interp(latitude=mask_lat, longitude=mask_lon, method='linear')\n",
    "\n",
    "            year = int(forecast_year)\n",
    "            month = int(forecast_month)\n",
    "\n",
    "            _, num_days = calendar.monthrange(year, month)\n",
    "\n",
    "            for mask_var in mask_variables:\n",
    "                mask = mask_ds.variables[mask_var][:]\n",
    "                \n",
    "                total_pcp = (np.sum(pcp_remap * mask * area)) * 4 * num_days # pcp is for 6 hours - convert to monthly\n",
    "                pcp_mm = total_pcp / np.sum(mask * area) # kg/m2 or mm\n",
    "\n",
    "                # Add data to the row\n",
    "                pcp_row.append(pcp_mm.data)\n",
    "\n",
    "            # Save the entire row to the CSV file\n",
    "            pcp_writer.writerow(pcp_row)\n",
    "\n",
    "        # Delete index file to avoid issues in the future if grib files are reopened\n",
    "        os.remove(index_file)\n",
    "        print(f'Done with {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directory to download files to if it does not already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/Users/fitzpatrick/Desktop/cnbs-predictor-1/data/input/CFS/' created.\n"
     ]
    }
   ],
   "source": [
    "create_directory(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the csv files or create new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fitzpatrick/Desktop/cnbs-predictor-1/data/input/CFS_TMP_forecasts_Avgs_K.csv opened for appending.\n",
      "/Users/fitzpatrick/Desktop/cnbs-predictor-1/data/input/CFS_EVAP_forecasts_Avgs_MM.csv opened for appending.\n",
      "/Users/fitzpatrick/Desktop/cnbs-predictor-1/data/input/CFS_PCP_forecasts_Avgs_MM.csv opened for appending.\n"
     ]
    }
   ],
   "source": [
    "tmp_csv, tmp_writer = open_csv(tmp_file, mask_variables, mode='a')\n",
    "evap_csv, evap_writer = open_csv(evap_file, mask_variables, mode='a')\n",
    "pcp_csv, pcp_writer = open_csv(pcp_file, mask_variables, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows the user to set the script to auto. When auto = yes, the script opens one of the CSVs (temperature), reads the last date that it recorded and automatically makes the start date the next day. It then will run through yesterday's date in order to be caught up. If auto = no, then the user can input a date range. This option is convienent for testing or for starting new CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fitzpatrick/Desktop/cnbs-predictor-1/data/input/CFS_TMP_forecasts_Avgs_K.csv opened for reading.\n"
     ]
    }
   ],
   "source": [
    "if auto == 'yes':\n",
    "    tmp = open_csv(tmp_file, mask_variables, mode='r')\n",
    "    last_cfs = str(tmp['cfs_run'].iloc[-1])[:8]\n",
    "    start_date_i = datetime.strptime(last_cfs, '%Y%m%d') + timedelta(days=1)\n",
    "    # Pull all the forecasts days up to yesterday (the most complete forecast)\n",
    "    end_date_i = datetime.now() - timedelta(days=1)\n",
    "\n",
    "else:\n",
    "    start_date_i = datetime.strptime(start_date, \"%Y-%m-%d\") # User input from above\n",
    "    end_date_i = datetime.strptime(end_date, \"%Y-%m-%d\") # User input from above\n",
    "\n",
    "    # Check if start_date is equal to or after end_date\n",
    "    if start_date_i == end_date_i:\n",
    "        print(\"The csv files are up-to-date.\")\n",
    "        exit()  # Stop the script\n",
    "    elif start_date_i > end_date_i:\n",
    "        print(\"There is an error in the input dates. Please try again.\")\n",
    "    else:\n",
    "        print(f\"Starting from: {start_date_i.strftime('%Y-%m-%d')} and continuing through: {end_date_i.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create a date range\n",
    "date_array = pd.date_range(start=start_date_i, end=end_date_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the mask file. Pull the latitude and longitude to be used to cut the global variable down to just the Great Lakes domain and upscale. Also calculates area of each of the grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the mask file and calculate the grid cell areas\n",
    "mask_ds = nc.Dataset(mask_file)\n",
    "mask_lat = mask_ds.variables['latitude'][:]\n",
    "mask_lon = mask_ds.variables['longitude'][:]\n",
    "area = calculate_grid_cell_areas(mask_lon, mask_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin loop to go through the user input dates. Loop creates a directory to download the CFS grib files, runs through the download_grb2_aws funtion to download and then run through the process_grib_files to do the calculations. It then saves the calculations to the CSV files, deletes the grib2 files and moves on to the next date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_array:\n",
    "    print(f\"Beginning Files for {date}.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    YYYY = date.strftime(\"%Y\")\n",
    "    YYYYMM = date.strftime(\"%Y%m\")\n",
    "    YYYYMMDD = date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    date = date.strftime('%Y%m%d')\n",
    "    download_path = f'{download_dir}{date}/CFS/'\n",
    "\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # Download the grib2 files using AWS or NCEI\n",
    "    if download_cfs == 'yes':\n",
    "        for utc_time in utc:\n",
    "            for product in products:\n",
    "                if source == 'aws':\n",
    "                    url_path = f'cfs.{date}/{utc_time}/monthly_grib_01/'\n",
    "                    download_grb2_aws(product, bucket_name, url_path, download_path)\n",
    "                elif source == 'ncei':\n",
    "                    base_url = 'https://www.ncei.noaa.gov/data/climate-forecast-system/access/operational-9-month-forecast/monthly-means/'\n",
    "                    url_path = f'{base_url}/{YYYY}/{YYYYMM}/{YYYYMMDD}/{YYYYMMDD}{utc_time}/'\n",
    "                    if not url_path or not check_url_exists(url_path):\n",
    "                        print(f\"No files available for {date}.\")\n",
    "                    else:\n",
    "                        download_grb2_ncei(product, url_path, download_path)\n",
    "                else:\n",
    "                    print('Input source does not exist. Source must be aws or ncei.')\n",
    "    \n",
    "    if process_cfs == 'yes':\n",
    "        print('process = yes')\n",
    "        process_grib_files(download_path, tmp_writer, evap_writer, pcp_writer, mask_lat, mask_lon, mask_ds, mask_variables, area)\n",
    "\n",
    "        if delete_files == 'yes':\n",
    "            delete_directory(download_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Elapsed time: {elapsed_time} seconds\")\n",
    "    \n",
    "    print(f'Done with {date}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close any open files before finishing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
