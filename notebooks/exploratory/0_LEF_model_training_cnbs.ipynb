{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Script\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "12/18/2024\n",
    "\n",
    "This script reads in CFSR data from 1979 - 2010 and trains machine learning models to target CNBS from L2SWBM across the 5 Great Lakes simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ConstantKernel, RBF, Matern, RationalQuadratic, ExpSineSquared\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import joblib\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the directory where you cloned the repo\n",
    "path_to_repo = '/Users/fitzpatrick/Desktop/'\n",
    "\n",
    "dir = path_to_repo + 'cnbs-predictor-1/data/input/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_in_month(year, month):\n",
    "    # Number of days in the month\n",
    "    num_days = calendar.monthrange(year, month)[1]\n",
    "    # Convert days to seconds\n",
    "    return num_days * 24 * 60 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_variables(df, lag=0, lead=0):\n",
    "    \"\"\"\n",
    "    Create the variables columns to include lags (last month values) and lead variables\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "    - lag (int): The number of months you want to include lagged variables. Default = 0\n",
    "    - lead (int): The number of months for the advance variables. Default = 0\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with added variable columns for lags and leading.\n",
    "    \"\"\"\n",
    "    df = df.copy()  # To avoid modifying the original DataFrame\n",
    "\n",
    "    new_columns = []  # List to store the new lag and lead columns\n",
    "\n",
    "    # Generate target columns for the lag and lead months\n",
    "    for column in df.columns:\n",
    "        for lag_month in range(1, lag + 1):\n",
    "            new_columns.append(df[column].shift(lag_month).rename(f'{column}_mo-{lag_month}'))\n",
    "        for lead_month in range(1, lead + 1):\n",
    "            new_columns.append(df[column].shift(-lead_month).rename(f'{column}_mo{lead_month}'))\n",
    "\n",
    "    # Concatenate the new columns with the original DataFrame\n",
    "    df = pd.concat([df] + new_columns, axis=1)\n",
    "\n",
    "    # Drop rows with any NaN values generated by shifting for the target\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in PCP data from CFSR [mm]\n",
    "data_1 = pd.read_csv(dir+'training/CFSR_APCP_Basin_Avgs_05.csv',sep=',')\n",
    "\n",
    "## Read in EVAP data from CFSR [mm]\n",
    "data_2 = pd.read_csv(dir+'training/CFSR_EVAP_Basin_Avgs_031.csv',sep=',')\n",
    "\n",
    "## Read in TMP data from CFSR [K]\n",
    "data_3 = pd.read_csv(dir+'training/CFSR_TMP_Basin_Avgs_031.csv',sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in L2SWBM in [mm]\n",
    "\n",
    "https://zenodo.org/records/13883098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_evap = pd.read_csv(dir + 'l2swbm/superiorEvap_MonthlyRun.csv')\n",
    "sup_runoff = pd.read_csv(dir + 'l2swbm/superiorRunoff_MonthlyRun.csv')\n",
    "sup_precip = pd.read_csv(dir + 'l2swbm/superiorPrecip_MonthlyRun.csv')\n",
    "\n",
    "eri_evap = pd.read_csv(dir + 'l2swbm/erieEvap_MonthlyRun.csv')\n",
    "eri_runoff = pd.read_csv(dir + 'l2swbm/erieRunoff_MonthlyRun.csv')\n",
    "eri_precip = pd.read_csv(dir + 'l2swbm/eriePrecip_MonthlyRun.csv')\n",
    "\n",
    "ont_evap = pd.read_csv(dir + 'l2swbm/ontarioEvap_MonthlyRun.csv')\n",
    "ont_runoff = pd.read_csv(dir + 'l2swbm/ontarioRunoff_MonthlyRun.csv')\n",
    "ont_precip = pd.read_csv(dir + 'l2swbm/ontarioPrecip_MonthlyRun.csv')\n",
    "\n",
    "mih_evap = pd.read_csv(dir + 'l2swbm/miHuronEvap_MonthlyRun.csv')\n",
    "mih_runoff = pd.read_csv(dir + 'l2swbm/miHuronRunoff_MonthlyRun.csv')\n",
    "mih_precip = pd.read_csv(dir + 'l2swbm/miHuronPrecip_MonthlyRun.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the data for training and testing. We set the features 'X' as total over lake\n",
    "precipitation, total over lake evaporation, and the average air temperature over each lake. The\n",
    "targets 'y' are RNBS for each lake simultaeously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = pd.DataFrame({\n",
    "    'sup_pcp_w': data_1['sup_lake'],\n",
    "    'eri_pcp_w': data_1['eri_lake'],\n",
    "    'ont_pcp_w': data_1['ont_lake'],\n",
    "    'mih_pcp_w': data_1['mih_lake'],\n",
    "    'sup_pcp_l': data_1['sup_land'],\n",
    "    'eri_pcp_l': data_1['eri_land'],\n",
    "    'ont_pcp_l': data_1['ont_land'],\n",
    "    'mih_pcp_l': data_1['mih_land'],\n",
    "    'sup_evap_w': data_2['sup_lake'],\n",
    "    'eri_evap_w': data_2['eri_lake'],\n",
    "    'ont_evap_w': data_2['ont_lake'],\n",
    "    'mih_evap_w': data_2['mih_lake'],\n",
    "    'sup_evap_l': data_2['sup_land'],\n",
    "    'eri_evap_l': data_2['eri_land'],\n",
    "    'ont_evap_l': data_2['ont_land'],\n",
    "    'mih_evap_l': data_2['mih_land'],\n",
    "    'sup_tmp_w': data_3['sup_lake'],\n",
    "    'eri_tmp_w': data_3['eri_lake'],\n",
    "    'ont_tmp_w': data_3['ont_lake'],\n",
    "    'mih_tmp_w': data_3['mih_lake'],\n",
    "    'sup_tmp_l': data_3['sup_land'],\n",
    "    'eri_tmp_l': data_3['eri_land'],\n",
    "    'ont_tmp_l': data_3['ont_land'],\n",
    "    'mih_tmp_l': data_3['mih_land']\n",
    "})\n",
    "\n",
    "# Set the index by date\n",
    "X.set_index(pd.to_datetime(data_1[['year', 'month']].assign(day=1)), inplace=True)\n",
    "\n",
    "# Targets are the components of NBS (P, E, R)\n",
    "targets = pd.DataFrame({\n",
    "    'sup_evap_t': sup_evap['Median'],\n",
    "    'sup_pcp_t': sup_precip['Median'],\n",
    "    'sup_rnoff_t': sup_runoff['Median'],\n",
    "    'eri_evap_t': eri_evap['Median'],\n",
    "    'eri_pcp_t': eri_precip['Median'],\n",
    "    'eri_rnoff_t': eri_runoff['Median'],\n",
    "    'ont_evap_t': ont_evap['Median'],\n",
    "    'ont_pcp_t': ont_precip['Median'],\n",
    "    'ont_rnoff_t': ont_runoff['Median'],\n",
    "    'mih_evap_t': mih_evap['Median'],\n",
    "    'mih_pcp_t': mih_precip['Median'],\n",
    "    'mih_rnoff_t': mih_runoff['Median']\n",
    "})\n",
    "\n",
    "# Set the index of the targets\n",
    "targets.set_index(pd.to_datetime(eri_evap[['Year', 'Month']].assign(day=1)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_X = shift_variables(X ,lag=0, lead=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_targets = shift_variables(targets, lag=0, lead=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Targets: 12\n",
      "Number of Features: 24\n"
     ]
    }
   ],
   "source": [
    "# Make sure the indices/dates align after the shifts\n",
    "aligned_y = shifted_targets.loc[shifted_X.index]\n",
    "\n",
    "print(f'Number of Targets: {aligned_y.shape[1]}')\n",
    "print(f'Number of Features: {shifted_X.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into training and testing data sets. We could do it as a random 80/20 split\n",
    "but instead we set split the data set by date ranges. This can easily be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset by date ranges into training and testing sets\n",
    "train_start_date = '1979-01-01'\n",
    "train_end_date = '2004-12-01'\n",
    "# Testing dataset\n",
    "val_start_date = '2005-01-01'\n",
    "val_end_date = '2011-01-01'\n",
    "\n",
    "X_train = shifted_X[train_start_date:train_end_date]\n",
    "y_train = aligned_y[train_start_date:train_end_date]\n",
    "X_test = shifted_X[val_start_date:val_end_date]\n",
    "y_test = aligned_y[val_start_date:val_end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to standardize the data from 0-1 before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "X_test_scaled = x_scaler.fit_transform(X_test)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "Below we train different models using the same data and calculate the r squared values on the \n",
    "test data to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.8420223920088205\n",
      "Mean Squared Error: 0.15797760799117966\n"
     ]
    }
   ],
   "source": [
    "# Testing Different Kernels\n",
    "# Basic kernel using ConstantKernel: r2 = 0.8259\n",
    "#kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-2, 1e2))\n",
    "\n",
    "# Matt's optimal kernel: \n",
    "# # MVP r2 = 0.8159\n",
    "# # Geo mask r2: 0.8176\n",
    "# # \n",
    "kernel = 1.0 * Matern(nu=1.5) * RationalQuadratic()\n",
    "\n",
    "# Test to add a seasonality component: r2 = 0.8279\n",
    "#period = 3.0  # Period of the season\n",
    "#kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + ExpSineSquared(length_scale=1.0, periodicity=period, periodicity_bounds=(1e-2, 1e2))\n",
    "\n",
    "#kernel = 1.0 * ExpSineSquared(periodicity=12)\n",
    "\n",
    "#kernel = 1.0 * RBF() + 1.0 * Matern(nu=2.5) + 1.0 * RationalQuadratic()\n",
    "\n",
    "# Set up the model\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "gpr.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(gpr, dir +'GP_trained_model.joblib')\n",
    "joblib.dump(x_scaler, dir + 'x_scaler.joblib')\n",
    "joblib.dump(y_scaler, dir + 'y_scaler.joblib')\n",
    "\n",
    "# Predictions\n",
    "y_pred, sigma = gpr.predict(X_test_scaled, return_std=True)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test_scaled, y_pred)\n",
    "r_squared = r2_score(y_test_scaled, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r squared value for the model is 0.5129044536132797\n",
      "Mean Squared Error: 614.908055784716\n"
     ]
    }
   ],
   "source": [
    "## Random Forest Regressor Model:\n",
    "\n",
    "# Initialize RandomForestRegressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model, dir + 'RF_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 6831.1494 - val_loss: 6295.4204\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6645.6333 - val_loss: 6203.2036\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6553.0889 - val_loss: 5992.6641\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6358.4233 - val_loss: 5567.8203\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5769.0596 - val_loss: 4821.2827\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4925.3877 - val_loss: 3730.0469\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3651.0073 - val_loss: 2587.1331\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2534.3660 - val_loss: 2005.3083\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2007.4865 - val_loss: 1841.4731\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1789.7808 - val_loss: 1587.2484\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1518.7781 - val_loss: 1445.3087\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1408.9698 - val_loss: 1352.3883\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1287.9960 - val_loss: 1264.8617\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1199.6194 - val_loss: 1178.8859\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1114.1548 - val_loss: 1094.1639\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1031.5652 - val_loss: 1017.4808\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 963.4911 - val_loss: 953.5501\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 897.9122 - val_loss: 902.6902\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 837.5193 - val_loss: 861.9966\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 795.1933 - val_loss: 830.8519\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 767.8907 - val_loss: 809.9324\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 734.3502 - val_loss: 791.8083\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 737.4129 - val_loss: 779.0113\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 720.0447 - val_loss: 766.3720\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 704.6344 - val_loss: 757.1157\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 696.8148 - val_loss: 749.6590\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 677.0074 - val_loss: 742.7921\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 682.5580 - val_loss: 736.1732\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 683.7668 - val_loss: 730.4579\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 671.3083 - val_loss: 724.9913\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 656.4572 - val_loss: 720.9991\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 662.6989 - val_loss: 715.9609\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 656.1758 - val_loss: 709.9830\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 644.3358 - val_loss: 705.4543\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 648.8271 - val_loss: 702.0975\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 648.9918 - val_loss: 697.1772\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 634.8154 - val_loss: 692.1481\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 644.0914 - val_loss: 687.8123\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 627.7090 - val_loss: 683.0498\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 627.1061 - val_loss: 678.7889\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 610.0682 - val_loss: 673.0208\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 611.5909 - val_loss: 669.0272\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 611.7651 - val_loss: 665.1396\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 615.9126 - val_loss: 659.8516\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 608.6925 - val_loss: 656.3388\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 607.8843 - val_loss: 652.5828\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 592.3854 - val_loss: 648.7169\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 593.1316 - val_loss: 643.4413\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 594.8934 - val_loss: 642.4845\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 590.0181 - val_loss: 638.9366\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "The r squared value for the model is 0.4825346767902374\n",
      "Mean Squared Error: 650.0106065339897\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(144)  # **** Number of targets ****\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')  # Using mean squared error (mse) as the loss function\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model,dir + 'NN_trained_model.joblib')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"The r squared value for the model is {r_squared}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
