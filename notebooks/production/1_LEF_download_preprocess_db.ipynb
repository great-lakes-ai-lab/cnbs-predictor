{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/Pre-process CFS forecast data\n",
    "Lindsay Fitzpatrick\n",
    "ljob@umich.edu\n",
    "08/28/2024\n",
    "Updated: 03/14/2024\n",
    "\n",
    "This script downloads CFS forecast data as GRIB2 files from either AWS or NCEI. It then processes these files to calculate key atmospheric metrics, including total precipitation, evaporation, and the average 2m air temperature over both lake and land areas for the Great Lakes. The results are then added to an existing or new database. To run this script, the following files are required:\n",
    "\n",
    "- GL_mask.nc\n",
    "- cfs_forecast_data.db (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the src directory (two levels up)\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "from src.database_utils import *\n",
    "from src.data_processing import create_directory, process_grib_files\n",
    "from src.hydro_utils import calculate_grid_cell_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the repository is cloned\n",
    "path_to_repo = '/Users/ljob/Desktop/'\n",
    "\n",
    "# Path to save downloaded data\n",
    "download_dir = path_to_repo + 'cnbs-predictor/data/CFS/'\n",
    "\n",
    "# Path to the directory containing input files\n",
    "input_dir = path_to_repo + 'cnbs-predictor/data/input/'\n",
    "\n",
    "# Path the GL mask file\n",
    "mask_file = input_dir + 'GL_mask.nc'\n",
    "\n",
    "# Path to the CFS forecast data database\n",
    "database = input_dir + 'cfs_forecast_data.db'\n",
    "\n",
    "# Data source: specify either 'aws' or 'ncei'\n",
    "source = 'aws'\n",
    "\n",
    "# Do you need to download CFS data? ('yes' or 'no')\n",
    "download_cfs = 'yes'\n",
    "\n",
    "# Do you want to process the CFS data? ('yes' or 'no')\n",
    "process_cfs = 'yes'\n",
    "\n",
    "# Should grib files be deleted after processing? ('yes' or 'no')\n",
    "delete_files = 'no'\n",
    "\n",
    "# Auto mode will automatically open the existing database, pull the last entered date to determine the start date, \n",
    "# and set the end date to yesterday, making the database 'up-to-date'. If 'no', you can manually enter a start and \n",
    "# end date (ideal for testing or if you need to redownload/reprocess specific time frames).\n",
    "auto = 'yes'\n",
    "\n",
    "# Specify the start and end dates if auto mode above is set to 'no'\n",
    "start_date = '10-29-2024'\n",
    "end_date = '10-31-2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presets\n",
    "\n",
    "These settings should remain unchanged unless the CFS data location changes, the user requires different files (where 'products' specifies the file prefix where different variables are stored), or a specific forecast time is needed (as indicated by utc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Presets ##\n",
    "products = ['pgb','flx']\n",
    "utc = ['00','06','12','18']\n",
    "\n",
    "# Define mask variables\n",
    "mask_variables = ['eri_lake','eri_land',\n",
    "                  'ont_lake','ont_land',\n",
    "                  'mih_lake','mih_land',\n",
    "                  'sup_lake','sup_land']\n",
    "\n",
    "#AWS bucket name to locate the CFS forecast\n",
    "bucket_name = 'noaa-cfs-pds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the download directory to see if it exists or create it if it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/Users/ljob/Desktop/cnbs-predictor/data/CFS/' already exists.\n"
     ]
    }
   ],
   "source": [
    "create_directory(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open existing Database or create new one if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<sqlite3.Connection at 0x128c19f30>, <sqlite3.Cursor at 0x128d15cc0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_cfs_db(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section allows the user to set the script to auto. When auto = yes, the script opens one of the CSVs (temperature), reads the last date that it recorded and automatically makes the start date the next day. It then will run through yesterday's date in order to be caught up. If auto = no, then the user can input a date range. This option is convienent for testing or for starting new CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from: 04-03-2025 00Z and continuing through: 04-27-2025 18Z\n"
     ]
    }
   ],
   "source": [
    "if auto == 'yes':\n",
    "        # Fetch next cfs_run date and use yesterday's date for the end date\n",
    "        start_date_i = get_next_cfs_run(database, 'cfs_forecast_data')\n",
    "        end_date_i = (datetime.now() - timedelta(days=1)).strftime(\"%m-%d-%Y\") + \" 18\"\n",
    "        # Validate dates\n",
    "        if start_date_i >= end_date_i:\n",
    "            print(\"The csv files are up-to-date.\")\n",
    "        else:\n",
    "            print(f\"Starting from: {start_date_i}Z and continuing through: {end_date_i}Z\")\n",
    "\n",
    "else:\n",
    "    # Ensure both start_date and end_date have hour info\n",
    "    start_date = (start_date + \" 00\") if len(start_date) == 10 else start_date\n",
    "    end_date = (end_date + \" 18\") if len(end_date) == 10 else end_date\n",
    "\n",
    "    # Convert to datetime objects for comparison\n",
    "    start_date_i = datetime.strptime(start_date, \"%m-%d-%Y %H\")\n",
    "    end_date_i = datetime.strptime(end_date, \"%m-%d-%Y %H\")\n",
    "\n",
    "    # Validate dates\n",
    "    if start_date_i == end_date_i:\n",
    "        print(start_date_i)\n",
    "        print(\"The csv files are up-to-date.\")\n",
    "    elif start_date_i > end_date_i:\n",
    "        print(start_date_i)\n",
    "        print(\"There is an error in the input dates. Please try again.\")\n",
    "    else:\n",
    "        print(f\"Starting from: {start_date_i.strftime('%m-%d-%Y %H')}Z and continuing through: {end_date_i.strftime('%m-%d-%Y %H')}Z\")\n",
    "\n",
    "date_array = pd.date_range(start=start_date_i, end=end_date_i, freq='6h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the mask file. Pull the latitude and longitude to be used to cut the global variable down to just the Great Lakes domain and upscale. Also calculates area of each of the grid cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the mask file and calculate the grid cell areas\n",
    "mask_ds = nc.Dataset(mask_file)\n",
    "mask_lat = mask_ds.variables['latitude'][:]\n",
    "mask_lon = mask_ds.variables['longitude'][:]\n",
    "area = calculate_grid_cell_areas(mask_lon, mask_lat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin loop to go through the user input dates. Loop creates a directory to download the CFS grib files, runs through the download_grb2_aws funtion to download and then run through the process_grib_files to do the calculations. It then saves the calculations to the CSV files, deletes the grib2 files and moves on to the next date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_array:\n",
    "    print(f\"Beginning Files for {date}.\")\n",
    "\n",
    "    YYYY = date.strftime(\"%Y\")\n",
    "    MM = date.strftime(\"%m\")\n",
    "    DD = date.strftime(\"%d\")\n",
    "    HH = date.strftime(\"%H\")\n",
    "\n",
    "    #date = date.strftime('%Y%m%d')\n",
    "    download_path = f'{download_dir}{YYYY}{MM}{DD}/'\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "\n",
    "    # Download the grib2 files using AWS or NCEI\n",
    "    if download_cfs == 'yes':\n",
    "        for product in products:\n",
    "            if source == 'aws':\n",
    "                url_path = f'cfs.{YYYY}{MM}{DD}/{HH}/monthly_grib_01/'\n",
    "                download_grb2_aws(product, bucket_name, url_path, download_path)\n",
    "            elif source == 'ncei':\n",
    "                base_url = 'https://www.ncei.noaa.gov/data/climate-forecast-system/access/operational-9-month-forecast/monthly-means/'\n",
    "                url_path = f'{base_url}/{YYYY}/{YYYY}{MM}/{YYYY}{MM}{DD}/{YYYY}{MM}{DD}{HH}/'\n",
    "                if not url_path or not check_url_exists(url_path):\n",
    "                    print(f\"No files available for {date}.\")\n",
    "                else:\n",
    "                    download_grb2_ncei(product, url_path, download_path)\n",
    "            else:\n",
    "                print('Input source does not exist. Source must be aws or ncei.')\n",
    "    \n",
    "    if process_cfs == 'yes':\n",
    "\n",
    "        process_grib_files(download_path, database, 'cfs_forecast_data', f'{YYYY}{MM}{DD}{HH}', mask_lat, mask_lon, mask_ds, mask_variables, area)\n",
    "\n",
    "        if delete_files == 'yes':\n",
    "            os.rmdir(download_path)\n",
    "    \n",
    "    print(f'Done with {date}.')\n",
    "print(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close any open files before finishing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
